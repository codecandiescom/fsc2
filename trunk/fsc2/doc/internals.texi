@c  $Id$
@c
@c  Copyright (C) 1999-2008 Jens Thoms Toerring
@c
@c  This file is part of fsc2.
@c
@c  Fsc2 is free software; you can redistribute it and/or modify
@c  it under the terms of the GNU General Public License as published by
@c  the Free Software Foundation; either version 2, or (at your option)
@c  any later version.
@c
@c  Fsc2 is distributed in the hope that it will be useful,
@c  but WITHOUT ANY WARRANTY; without even the implied warranty of
@c  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
@c  GNU General Public License for more details.
@c
@c  You should have received a copy of the GNU General Public License
@c  along with fsc2; see the file COPYING.  If not, write to
@c  the Free Software Foundation, 59 Temple Place - Suite 330,
@c  Boston, MA 02111-1307, USA.


@node Internals, Modules, Cloning Devices, fsc2
@chapter Internals


In this chapter I will try to explain in some more detail how
@code{fsc2} works internally. I hope that this will help especially if
you're going try to write a new module for a device (or if you get upset
about things @code{fsc2} does differently from the way you would like it
to).


As you will already have understood, @code{fsc2} is basically an
interpreter for @code{EDL} scripts. Traditional interpreters interpret
each line of input one after another, i.e.@: they analyze a line, and,
if the line is syntactically correct, execute it and then continue with
the next one. That's what @code{fsc2} also does, but only for the parts
before the @code{EXPERIMENT} section. In contrast, the interpretation of
the @code{EXPERIMENT} section of an @code{EDL} file consists of several
steps. In the first step the @code{EXPERIMENT} section as a whole is
read in and only syntax checks are done. In the second step a complete
test run of @code{EXPERIMENT} section of the script is done to avoid that
the experiment will have to be stopped due to obvious logical errors in
the @code{EDL} script. The device modules are involved, i.e.@: they
already can do all kinds of checks on what can be expected to happen
during the real experiment and detect possible problems.


Only if all these tests succeeded the third step, the execution of the
@code{EXPERIMENT} section of the @code{EDL} script, is started, i.e.@:
the experiment is run. When an experiment is restarted (without
reloading the @code{EDL} file) only this third step is repeated.


In the following I will try to explain what happens during these three
steps of the execution of an @code{EDL} script in some more details. Then
follows a real long section with a @i{tour de force} through the sources
of the program. This is hopefully going to be helpful for people that try
eliminate bugs or even extend @code{fsc2}.

@ifnottex

@menu
* First stage of interpretation::
* Second stage of interpretation::
* Third stage of interpretation::
* Reading the sources::
* Coding conventions::
@end menu

@end ifnottex


@node First stage of interpretation, Second stage of interpretation, Internals, Internals
@section First stage of interpretation


In the first stage of the interpretation the script is read in a token
by token fashion, where tokens are e.g.@: variable or function names,
numbers, braces, semicolons, section labels etc. This is mainly done by
the code within the files which you'll find in the @file{src} directory
having an extension of @code{.l}. As you will see there's such a file
for each of the different sections an @code{EDL} script may contain.
Actually, there are some extra ones, @file{split_lexer.l} is the central
switch for the first stage that calls the individual tokenizers in turn
on finding a new section label and also deals with error conditions that
can't be handled by the section interpreters. Two extra tokenizers,
@file{devices_list_lexer.l} and @file{func_list_lexer.l} are for analyzing
the files with the list of device modules, @file{conf/Devices}, and the
list of functions, @file{conf/Functions} (they also contain some additional
@code{C} code for interpreting the tokens).


Finally, there is @file{fsc2_clean.l}. @code{EDL} itself expects its
input to be a single file in a certain format. Thus it does not deal
directly with the input files (there can be more than one when there are
@code{#INCLUDE} statements in an @code{EDL} script). Therefore, there's
an extra program, @file{fsc2_clean} that translates the input @code{EDL}
script into a form that the main program understands. @code{fsc2_clean}
for example removes all comments, includes files for @code{#INCLUDE}
statements, deals with units, adds information about line numbers and
file names, deals with physical units etc.@:, and then passes this
cleaned-up input to @code{fsc2}. If you are interested what @code{fsc2}
really sees of an @code{EDL} file you can run @code{fsc2_clean} with the
@code{EDL} file as its standard input, i.e.@:
@example
fsc2_clean < edl_script.edl
@end example
@noindent
(please note that the output of @code{fsc2_clean} program contains some
non-printable characters).


The lines of @code{EDL} script in the sections preceeding the
@code{EXPERIMENT} section are executed immediately. E.g.@: during the
handling of the @code{DEVICES} section the modules for the listed
devices are loaded and the functions defined in the modules are included
into @code{fsc2}s internal list of functions that can be used from
within the @code{EDL} script. While reading the @code{VARIABLES} section
the newly defined variables are added to @code{fsc2}s list of variables,
and, if necessary, initialized.


While the tokenizers (i.e.@: the files with an extension of @code{.l})
are used for splitting of the input into manageable tokens, the
execution of the code (now consisting of a stream of tokens) is done in
the files with an extension of @code{.y} (or, to be precise, by the code
generated from these files). In these files, the parsers, actions
(mostly a few lines of @code{C} code) are executed for syntactically
correct sets of tokens. Because actions can only be executed for input
with valid syntax, these files practically define what is syntactically
correct and what is not (and since the syntax differs a bit for the
different sections there's a parser for each of the sections).


To give you an example, here's a very simple statement from an
@code{EDL} script:
@example
a = B_x + 3;
@end example
The tokenizer doesn't has too much to do in this case, it will output a
list of the tokens of this line, together with some information about the
class the individual tokens belong to. So, it will pass the following
kind of information to the parser:
@example
Floating point variable named 'a'
Equal operator
Integer variable named 'B_x'
Plus operator
Integer number with a value of 3
End of statement character: ;
@end example
@noindent
The parser, in turn, has a list of all syntactically correct
statements@footnote{Actually, the parser does not really has a list of
all syntactically correct statements but contains a set of rules that
define exactly how such statements may look like. One of these rules for
example is that a variable name and an equal operator may be followed by
either a variable, a function call or an integer or floating point
number. Anything not fitting this pattern is a syntax error.}, together
with the information what to do for these statements. One of the rules
is that a statement consisting of sequence of the tokens
@example
Variable, Equal operator, Variable, Plus operator,
integer number, end of statement character
@end example
@noindent
is syntactically correct and that for this sequence of tokens some
@code{C} code has to be executed that fetches the value of the variable
@code{B_x}, adds it to the integer number and finally stores the result
into the variable @code{a}. Statements that are not in the parsers list
are @i{per definitionem} syntactically incorrect. For example, there is
no rule on how to deal with a sequence of tokens as the one above but
with the number @code{3} at the end missing. Because the parser looks at
the statements token by token it won't complain while getting the first
four tokens up to and including the plus operator. Only if the end of
statement operator, the semicolon, is found directly after the plus sign
it will recognize that there's no rule on how to deal with the situation,
print the error message @code{Syntax error near token ';'} (plus the file
name and line number) and abort.


The @code{EXPERIMENT} section is handled differently. Most important,
the code of the @code{EXPERIMENT} section is not executed at this
stage. It is just split up into its tokens and only some rudimentary
syntax check is done, e.g.@: undefined variables or mismatched braces
etc.@: are detected. Instead an internal list of all the tokens the
@code{EXPERIMENT} section consists of is created. This list is later
used to test and execute the @code{EXPERIMENT} section.


Writers of modules should know that the modules already get loaded when
the @code{DEVICES} section (which always must be the first section) is
dealt with. A module may contain a special function, called a hook
function, that automatically gets called when the module has just been
loaded.  This allows for example to set the internal variables of the
module to a well-defined state. This function may not call any functions
accessing the device because neither the GPIB bus nor the serials ports
(or any other devices like ISA or PCI cards) are configured at this
moment.


While handling the part of the @code{EDL} script up to the start of the
@code{EXPERIMENT} section, only thar functions from the modules may be
called that have been explicitely declared to be usable already before
the start of the @code{EXPERIMENT} section (and then are not allowed to
talk to the devices during that stage). Usually such function calls will
be used to define the state of the device at the start of the experiment.
For example, the
@code{PREPARATIONS} section may contain
a line like
@example
lockin_sensitivity( 100 uV );
@end example
@noindent
When @code{fsc2} interprets this line it will call the appropriate
function in the module for the lock-in amplifier with a floating point
number of @code{0.0001} as the argument (the module does not have to
take care of dealing with units, they are already translated by
@code{fsc2}, or, to be precise, by @code{fsc2_clean}).  The module
function for setting the lock-in amplifiers sensitivity should now check
the argument it got passed (there may or may not be a sensitivity
setting of @code{0.0001} and only the module knows about this). If the
argument is reasonable the module should store the value as to be set
when the lock-in amplifier finally gets initialized at the start of the
experiment.


How to deal with wrong arguments or arguments that don't fit (e.g.@: if
the argument is @code{40 uV} but the lock-in amplifier has only
sensitivity settings of @code{30 uV} and @code{100 uV}) is completely up
to the writer of the module, @code{fsc2} will accept whatever the module
returns. For example the module may accept the argument after changing
it to something to the next possible sensitivity setting and printing
out a warning or it may bail out and tell @code{fsc2} to stop interpreting
the @code{EDL} script.


Another thing module writers should keep in mind is that this first (and
also the second) stage is only run once, while the experiment itself may
be run several times. Thus it is important that the values with which a
device must be initialized at the start of an experiment are stored in a
way that they aren't overwritten during the experiment. For example, it
does not suffice to have one single variable for the lock-in amplifiers
sensitivity because the sensitivity and thus the variable might get
changed during the experiment.


@node Second stage of interpretation, Third stage of interpretation, First stage of interpretation, Internals
@section Second stage of interpretation


The second stage of the interpretation of an @code{EDL} script is
the test run of the @code{EXPERIMENT} section. A test run is necessary
for two reasons. First, only a very rudimentary syntax check has been
done for the @code{EXPERIMENT} section until now. Second, and much more
important, the script may contain logical errors and it would be rather
annoying if these would only be found after the experiment had already
been run for several hours, necessitating the premature end of the
experiment. For example, without a "dry" run it could happen that only
after a long time it is detected that the field of the magnet is
requested to be set to a value that the magnet can't produce. In this
case there usually are few alternatives, if any, to aborting the
experiment. Foreseeing and taking the appropriate measures for such
possibly fatal situation would complicate both the writing of modules
and @code{EDL} scripts enormously and probably would still not catch
all of them.


On the other hand, by doing a test run for example the function for
setting the magnet to a new field will be called with all values that
are to be expected during the real experiment and thus invalid field
settings can be detected ilready in this "dry" run. Doing a test run
is much faster than running the experiment itself because during the
test run the devices will not be accessed (which usually uses at least
90% of the whole time), calls of the @code{wait()} function do not make
the program sleep for the requested time, no graphics are drawn etc.


The writers of modules have an important responsibility to make running
the test run possible. During the test run the devices can't be
accessed. Despite that the modules have to deal in a reasonable way with
requests for returning data from the devices. Thus the modules must,
during the test run, "invent" data for the real ones. This can be a bit
tricky and special care must be taken to insure that these "invented"
data are consistent. For example, if a module for a lock-in amplifier
first gets asked for the sensitivity setting and then for measured data
it may not return data that represent voltages larger than the
sensitivity setting it "invented". There may even be situations where
the module has no chance to find out if the arguments it gets passed for
a function are acceptable without determining the real state of the
device. If possible, incidents like this should be stored by the module
and the module should test at the time of device initialization if these
arguments were really acceptable and, if not, stop the experiment.


A typical example of this case are the settings for a "window" for the
digitizers, defining the part of a curve that gets returned or that is
integrated over etc. Because during the test run neither the timebase
nor the amount of pre-trigger the digitizer is set to are known (unless
both have been set explicitely from the @code{EDL} script) it can't be
tested if the windows start and end positions are within the time range
the digitizer measures. Thus the module can just store these settings
and report to @code{fsc2} that they seem to be reasonable. Only when the
experiment starts and the module has its first chance of finding out the
timebase and pre-trigger setting it can do the necessary checks on the
windows settings and should abort the experiment at the earliest
possible point if necessary.


To make things a bit easier when writing modules hook functions can be
defined within a module that get called automatically at the start of
the test run and after the test run finished successfully.


@node Third stage of interpretation, Reading the sources, Second stage of interpretation, Internals
@section Third stage of interpretation


In the third and final stage of the interpretation of an @code{EDL}
script the real experiment is run. This third stage may be repeated
several times if the user restarts an experiment without reloading the
@code{EDL} file.


At the start of the third stage the GPIB bus, the serial ports, the
LAN connections and the RULBUS printer port are initialized (at least
if one of the devices needs them). Next the hook functions in the
modules are called that allow the modules to initialize the devices
and do all checks they find necessary. If this was successful the
graphics for the experiment are initialized, opening up the display
windows. When all this has been done @code{fsc2} is ready to do the
experiment, i.e.@: to interpret the @code{EXPERIMENT} section.


But there is a twist. Just before starting to interpret the
@code{EXPERIMENT} section @code{fsc2} splits itself into two independent
processes by calling @code{fork()}. If you use the @code{ps} command to
list all your running processes suddenly a new instance of @code{fsc2}
will be listed@footnote{Please note that already before the experiment
gets started you will find that there are three instances of @code{fsc2}
running, during the experiment there are (at least) four.}. The new
processes is doing the interpretation of the @code{EXPERIMENT} section,
i.e.@: is running the experiment, while one of the other processes is
responsible for the graphics and all interactions with the user.


The main reason for splitting the execution of the experiment into two
separate tasks is the following: the execution of the experiment, as far
as concerned with acquiring data from the devices etc.@: should be
unimpeded (at least as far as possible) from the task of dealing with
displaying data and user requests to allow maximum execution speed and
to make the timing of the experiment less dependent on user
interruptions. Take for example the case that the user starts to move
one of @code{fsc2}s windows around on the screen. As long as she is
moving the window no other instructions of the program get executed,
which effectively would stop the experiment for this time even though
nothing really relevant happens. By having one task for the actual
execution of the experiment and one for the user interaction this
problem vanishes because the task for the experiment can continue while
only the other task, responsible for the user interaction, is
blocked. This, of course, also applies to all other actions the user may
initiate, e.g.@: resizing of windows, magnification of data etc.


Another advantage is, of course, that on machines with more than one
processor the workload can be distributed onto two processors.


The approach requires some channels of communication between the two
processes. Because the user interaction task has to draw the new data
the other task, executing the experiment, is producing the data will have
to passed on from the experiment task to the user interaction task. And,
the other way round, the user interaction task must be able to send back
informations received from the user (e.g.@: which file name got selected)
and to stop the experiment when the user hits the @code{Stop}
button. Care has been taken that this is done in a way that usually
can't be impeded by user interventions. The only exceptions are cases
where the further execution of the experiment depends on user input,
e.g.@: if within the experiment a new file has to be opened and the name
must be selected by the user.


The most important part of the communication between parent process (the
user interaction task) and the child process (the task running the
experiment) is basically a one-way communication -- the child process
must pass on newly acquired data to the parent process to be drawn. The
child processes writes the new data (together with the information where
they are to be drawn) into a shared memory segment and stores the key
for this memory segment in an unused slot in another buffer (that also
resides in shared memory). Then it sends the parent process a signal to
inform it that new data are available and continues immediately.


The user interaction process gets interrupted by the signal (even
while it is doing some other tasks on behalf of the user), removes and
stores the key for the memory segment, and can now deal with the new
data whenever it has the time to do so.


Problems can arise only if the child process for running the experiment
creates new data at a much higher rate than the parent can accept them,
in which case the buffer for memory segment keys would fill
up@footnote{The buffer is guarded against overflows by a semaphore that
is initialized to the number of slots in the buffer and on which the
child process does a down operation before writing data into the buffer
while the parent process posts it after removing an item.}. Only if all
the slots in the buffer are used up child process will have to suspend
the experiment until the parent empties one or more of the slots. But
fortunately in practice this rarely happens. And as a further safeguard
against this happening the parent is written in a way that it will empty
slots in the buffer as fast as possible, if necessary deferring to draw
data or to react to user requests.


There is also a second communication channel for cases where the task
running the experiment needs some user input. Typical cases are requests
for file names, but also requests for information about the state of
objects in the toolbox. Here the task running the experiment always has
to wait for a reaction by the user interaction task (which in turn may
have to wait for user input). This communication channel is realized by
a pair of pipes between the processes.



@node Reading the sources, Coding conventions, Third stage of interpretation, Internals
@section Reading the sources


The following tries to give you an introduction to where to look when
you are searching for something in the source code of @code{fsc2}. Of
course, the program has gotten too complex to be described easily (and
with less space then required for the program itself). Thus all I can
try is to show you the red line through the jungle of code, from what's
happening when @code{fsc2} is started, when an @code{EDL} script gets
loaded, tested and finally executed. This is still far from complete and
work in progress at best.


Lets start with what to do when you want to debug @code{fsc2}. It's
probably obvious that when you want to run the main (parent) process of
@code{fsc2} under a debugger you just start it within the debugger. To
keep the debugger from getting stopped each time an internally used
signal is received you probably should start with telling the debugger
to ignore the two signals @code{SIGUSR1} and @code{SIGUSR2}. Under
@code{gdb} you do this by entering
@example
(gdb) handle SIGUSR1 nostop noprint
(gdb) handle SIGUSR2 nostop noprint
@end example


Debugging the child process that runs the experiment requires the
debugger to attach to the newly created child process. To be able to do
so without the child process already starting to run the experiment while
you're still in the process of attaching to it you have to set the
environment variable @code{FSC2_CHILD_DEBUG}, e.g.@:
@example
jens@@crowley:~/Lab/fsc2> export FSC2_CHILD_DEBUG=1
@end example
@noindent
When this environment variable is defined (what you set it to doesn't
matter if it's not an empty string) the child process will @code{sllep()}
for about 10 hours or until it receives a signal, e.g.@: due to the debugger
attaching to it. Moreover, when @code{FSC2_CHILD_DEBUG} is set a line
telling you the PID of the child process is printed out when the child
process gets started. All you have to do is to start the debugger with
the PID to attach to. Here's an example of a typical session where I start
to debug the child process using @code{gdb}:
@example
jens@@crowley:~/Lab/fsc2 > export FSC2_CHILD_DEBUG=1
jens@@crowley:~/Lab/fsc2 > src/fsc2 &
[2] 28801
jens@@crowley:~/Lab/fsc2 > Child process pid = 28805
jens@@crowley:~/Lab/fsc2 > gdb src/fsc2 28805
GNU gdb 5.0
Copyright 2000 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB.  Type "show warranty" for details.
This GDB was configured as "i386-suse-linux"...
/home/jens/Lab/fsc2/28805: No such file or directory.
Attaching to program: /home/jens/Lab/fsc2/src/fsc2, Pid 28805
Reading symbols from /usr/X11R6/lib/libforms.so.1...done.
Loaded symbols for /usr/X11R6/lib/libforms.so.1
Reading symbols from /usr/X11R6/lib/libX11.so.6...done.
Loaded symbols for /usr/X11R6/lib/libX11.so.6
Reading symbols from /lib/libm.so.6...done.
Loaded symbols for /lib/libm.so.6
Reading symbols from /lib/libdl.so.2...done.
Loaded symbols for /lib/libdl.so.2
Reading symbols from /usr/local/lib/libgpib.so...done.
Loaded symbols for /usr/local/lib/libgpib.so
Reading symbols from /lib/libc.so.6...done.
Loaded symbols for /lib/libc.so.6
Reading symbols from /usr/X11R6/lib/libXext.so.6...done.
Loaded symbols for /usr/X11R6/lib/libXext.so.6
Reading symbols from /usr/X11R6/lib/libXpm.so.4...done.
Loaded symbols for /usr/X11R6/lib/libXpm.so.4
Reading symbols from /lib/ld-linux.so.2...done.
Loaded symbols for /lib/ld-linux.so.2
Reading symbols from /lib/libnss_compat.so.2...done.
Loaded symbols for /lib/libnss_compat.so.2
Reading symbols from /lib/libnsl.so.1...done.
Loaded symbols for /lib/libnsl.so.1
Reading symbols from /usr/lib/gconv/ISO8859-1.so...done.
Loaded symbols for /usr/lib/gconv/ISO8859-1.so
Reading symbols from /usr/local/lib/fsc2/fsc2_rsc_lr.fsc2_so...done.
Loaded symbols for /usr/local/lib/fsc2/fsc2_rsc_lr.fsc2_so
Reading symbols from /usr/local/lib/fsc2/User_Functions.fsc2_so...done.
Loaded symbols for /usr/local/lib/fsc2/User_Functions.fsc2_so
0x40698951 in __libc_nanosleep () from /lib/libc.so.6
(gdb) handle SIGUSR1 nostop noprint
(gdb) handle SIGUSR2 nostop noprint
(gdb)
@end example
@noindent
(There may be even more lines starting with "@code{Reading symbols for}"
and "@code{Loading symbols from}" if your @code{EDL} script lists some
modules in the @code{DEVICES} section.) Now the child process will be
waiting at the very start of its code in the function @code{run_child()}
in the file @file{run.c}.


Please note that because @code{fsc2} is normally running as a setuid-ed
process you must not try to debug the already installed and setuid-ed
version (that's not allowed for security reason) but only a version
which belongs to you and for which you have unlimited execution
permissions. This might require that you temporarily relax the
permissions on the device files (for the GPIB board, the serial ports
and, possibly, cards installed in the computer and used by @code{fsc2})
of devices that are controlled by the @code{EDL} script you use during
debugging to allow access by all users (or at least you). Don't forget
to reset the permissions when you're done.


This point out of the way I'm now going to start a @i{tour de force}
through the sources. When @code{fsc2} is invoked it obviously starts with
the @code{main()} function in the file @file{fsc2.c}. After setting up
lots of global variables and checking the command line options it tries to
connect to a kind of daemon process (or starts it if it's not already
running).  This daemon is taking care of situations where @code{fsc2} is
running in non-exclusive mode, i.e.@: when more than one instance of
@code{fsc2} is allowed to be run at the same time, and it will tell new
instances about what they are allowed to do and what not to avoid more
than one instance trying to access the same devices at the same time. It
also is supposed to remove things like lock files, shared memory
segments etc.@: should it ever happen that @code{fsc2} crashes so badly
that it isn't able anymore to clean up after itself (not that this is
supposed to happen;-).


When this hurdle has been taken the initialization of the graphics is
done. All the code for doing so is in the file @file{xinit.c}. You will
have to read a bit about the @code{Xforms} library to understand what's
going on there. Mostly it consists of loading a shared library for
creating the forms used by the program (there are two shared libraries,
@file{fsc2_rsc_lr.fsc2_so} and @file{fsc2_rsc_hr.fsc2_so}, which one is
loaded depends on the screen resolution and the comand line option
@code{-size}), evaluating the settings in the @file{.Xdefaults} and
@file{.Xresources} files, again setting up lots of global variables and
doing further checks on the command line arguments.


When this part was successful some further checks of the remaining
command line options are done and, if specified on the command line, an
@code{EDL} script is loaded. Now we're nearly ready  to start the main
loop of the program. But before this loop is entered another new process
is spawned that opens a socket (of type @code{AF_UNIX}, i.e.@: a socket
to which only processes on the same machine can connect) to listen on
incoming connections from external programs that want to send @code{EDL}
scripts to @code{fsc2} for execution. The code for spawning this child
process and the code for the child process itself can be found in the
@file{conn.c}.


After this stage the main loop of the program is entered. It consists of
just these two lines:
@example
while ( fl_do_forms( ) != GUI.main_form->quit )
    /* empty */ ;
@end example
Everything else is hidden behind these two lines. What they do is to
wait for new events until the @code{Quit} button gets pressed. Possible
events are clicking on the buttons in the different form, but they don't
need to be mentioned in this loop because all buttons trigger callback
functions when clicked on. The remaining stuff in the @code{main()}
function is just cleaning up when the program quits and a few things for
dealing with certain circumstances.


When you want to understand what's really going on you will have to
start with figuring out what happens in the callback functions for the
different buttons. The simplest way to find out which callback functions
are associated with which functions is probably to use the
@code{fdesign} program coming with the @code{Xforms} library and
starting it on one of the files @file{fsc2_rsc_lr.fd} or
@code{fsc2_rsc_hr.fd}. From within it you can display all of the forms
used by the program and find out the names of the callback functions
associated with each element of the forms.


The callback functions for the buttons of the main form are mostly in
@file{fsc2.c}. I will restrict myself to the most important ones: The
@code{Load} button invokes the function @code{load_file()}, which is
straight quite forward -- it asks the user to select a new file, checks
if it exists and can be read and, if this tests succeed, loads the file
and displays it in the main browser.


Once a file has been read in the @code{Test} button gets activated.
When it gets clicked on the function @code{test_file()} gets invoked and
that's were things get interesting. As you will find over and over again
in the program is starts with lots of testing and adjustments of the
buttons of the main form. (Should you worry what the lines like
@example
notify_conn( BUSY_SIGNAL );
@end example
@noindent
and
@example
notify_conn( UNBUSY_SIGNAL );
@end example
@noindent
are about: they tell the child process listening for external
connections that @code{fsc2} is at the moment too busy to accept new
@code{EDL} scripts and then that it's again prepared to load such a
script.)


The real fun starts at the line
@example
state = scan_main( EDL.in_file, in_file_fp );
@end example
@noindent
which calls the central subroutine to parse and test the @code{EDL}
script. A good deal of the following is going to be about what's
happening there.


The function @code{scan_main()} is located in the file
@file{split_lexer.l}. This obviously isn't a normall @code{C} file but a
file from which the @code{flex} utility creates a @code{C} file. If you
don't know yet, @code{flex} is a tool that generates programs that
perfom pattern-matching on input text, typically returning a different
value for each token, possibly with some more information about the
value of the token attached (i.e.@: an integer number could be a token
and it might a certain value to indicate that it found an integer number
as well as some extra information about the value that integer has).
That means that a program created by @code{flex} will dissect an input
text into the tokens according to the some rules given the @code{flex}
input file (in this case @file{split_lexer.l}) and execute some action
for each token found. And that's exactly what needs to be done with a
@code{EDL} script before it can later be digested by @code{fsc2} (with
the help of another tool, @code{bison}).


Before @code{scan_main()} starts tokenizing the input it does some
initalization of things that may be needed later on. This consists of
first setting up an internal list of built-in @code{EDL} functions and
@code{EDL} functions that might be supplied by modules; this is done by
calling @code{functions_init()} in the file @file{func.c}. Built-in
functions are all listed at the top of @file{func.c} and the list built
from it contains information about the names of the functions, the @code{C}
function that are to be called for the @code{EDL} functions, the number
of arguments, and in which sections of the program the functions are
allowed to be called. When @code{fsc2} is done with it's built-in
functions it also appends to the list the functions supplied by modules.
These are found in the @file{Functions} file in the @file{config}
subdirectory. To do so another @code{flex} tokenizer is invoked on this
file, which is generated by the code in @file{func_list_lexer.l}.


After assembling the list of functions @code{fsc2} also creates a list
of the registered modules. This is done by invoking the tokenizer
created from the file @file{devices_list_lexer.l} on the list of
all devices, @file{Devices} also in the @file{config} subdirectory.


When this succeeded @code{fsc2} is ready to start interpreting the input
@code{EDL} file. But there's a twist: it does not work directly with the
@code{EDL} file, but with a somewhat cleaned up version as has already
ben mentioned above. This cleaning up is done by invoking an external
utility, @code{fsc2_clean}, again a @code{flex} generated program from
the file @code{fsc2_clean.l}. This is done in the function
@code{filter_edl()} in @file{util.c}. The @code{fsc2_clean} utility is
started with its @code{stdin} redirected to the @code{EDL} input file
and its @code{stdout} redirected to a pipe, from which @code{fsc2} in
the following is reading the cleaned up version of the @code{EDL} file.


The tokenizer (or "lexer") created from @file{split_lexer.l} is rather
simple in that it just reads in the @code{EDL} code until it finds the
first section keyword (and this should be the first line the lexer gets
from @code{fsc2_clean}, which already removed all comments etc.). On
finding the first section keyword control is transfered immediately to
another lexer that is specifically written for dealing with the syntax
of this section. And that's why there are that many further files to
generate @code{flex} scanners, i.e.@: files with names ending in
@code{.l}, for each section there's a different tokenizer. These are
in the sequence the resulting lexers usually get invoked:
@example
devices_lexer.l        DEVICES section
vars_lexer.l           VARIABLES section
assign_lexer.l         ASSIGNMENTS section
phases_lexer.l         PHASES section
preps_lexer.l          PREPARATIONS section
exp_lexer.l            EXPERIMENT section
@end example
@noindent
Each of these lexers only returns to the one from @file{split_lexer.l}
when it finds a new section label (or when an error is detected).


But these lexers don't work alone. The lexers main job is to split up
the source in reasonably chunks. These would e.g. keywords, variable and
function names, numbers, arithmetic operators, parentheses, semicolons,
commas etc. But that's not enough to be able to understand what the
@code{EDL} script means. We also need a parser, that tries to make sense
from the stream of tokens created by the lexer by checking if the
sequences of tokens make up syntactically correct statements that then
get executed by calling some appropriate @code{C} code. These parsers
are created by another tool, @code{bison}, from files with names ending
in @code{.y}. These are
@example
devices_parser.y        DEVICES section
vars_parser.y           VARIABLES section
assign_parser.y         ASSIGNMENTS section
phases_parser.y         PHASES section
preps_parser.y          PREPARATIONS section
exp_test_parser.y       EXPERIMENT section
exp_run_parser.y        EXPERIMENT section
condition_parser.y      EXPERIMENT section
@end example
@noindent
Since the @code{EXPERIMENT} section is somewhat special so there's not
only one parser but three, which one is going to be used depends on the
circumstances.


If you don't know yet how lexers like @code{flex} and @code{lex} and
parsers like @code{bison} and @code{yacc} work and how they are combined
to interpret input you should start trying to find out, @code{fsc2}
strongly relies on them and you probably will have some of problems
understanding much of the sources without at least some basic knowledge
about them.


In a typical @code{EDL} script the first lexer getting involved is the
one for the @code{DEVICES} section, generated from
@file{devices_lexer.l}. This immediately calls the parser, generated
from @file{devices_parser.y}.  The lexer and parser are very simple
because all the @code{DEVICES} section may consist of is a list of
device names, separated by semicolons. The only thing of interest is
that when the end of the @code{DEVICES} section is reached it invokes
the function @code{load_all_drivers()} from the file @code{loader.c},
which is central to the plugin-like architecture of device handling in
@code{fsc2}.


The first part of @code{load_all_drivers()} consists of loading the
libraries for the devices listed in the @code{DEVICES} section (plus
another one called @code{User_Functions.fsc2_so}) and then trying to
find the (non-builtin) functions in the libraries that are listed in the
@file{Functions} file in the @file{config} subdirectory, which already
has been read in. This is done in the @code{load_functions} subroutine.
Here first a library gets loaded (using @code{dlopen(3)}), and if this
succeeds, the function tries to determine the addresses of the hook
functions (see the next chapter about writing modules for what the hook
functions are good for in detail, it should suffice to say that these
are (optional) functions in the modules that get executed at certain
points during the execution of the @code{EDL} script, i.e.@: after the
library has been loaded, before and after the test run, before and after
the start of the experiment and, finally, just before the modules gets
unloaded). Then @code{fsc2} runs through its list of non-builtin
functions and checks if some of them can be found in the library.


This last step is getting a bit more complicated by the fact that it is
possible to load two or more modules with the same type (e.g.@: two
modules for lock-in amplifiers), which both will supply functions of the
same names. @code{fsc2} recognizes this from a global variable, a string
with the device type, that each module is supposed to define. When it finds
that there are two or more devices with the same type (according to this
global variable), it will accept functions of the same name more than
one time and make the names unique by appending a hash ("@code{#}") and
a number for the device. So, if there are modules for two lock-in
amplifiers listed in the devices section, both supplying a function
@code{lockin_get_data()}, it will create two entries in its internal
list of non-builtin functions, one named @code{lockin_get_data#1()} and
associated with the first lock-in amplifier in the @code{DEVICES}
section and one named @code{lockin_get_data#2()} for the second
lock-in. The first, addressing the first lock-in, can then be called as
either @code{lockin_get_data#1()} (or also without the "@code{#1}"),
while for @code{lockin_get_data#2()} the function from the library for
the second lock-in amplifier is used.


After all device libraries have been loaded successfully the functions
@code{init_hook()} in all modules that have such a function are invoked,
always in the same sequence as they were listed in the @code{DEVICES}
section. The modules can use these hook function to initialize
themselves.


After this the work for of the @code{load_all_drivers()} and also the
lexer for the @code{DEVICES} section is done and control returns to the
lexer generated by @file{split_lexer.l} to the function
@code{section_parser()}. The last thing the lexer for the @code{DEVICES}
section did was setting a variable that tells this function what is the
next section in the @code{EDL} code. All the function now does is
transfer control to the lexer for that section.


Normally, the next section will be the @code{VARIABLES} section and the
lexer and parser, generated from @code{vars_lexer.l} and
@file{vasrs_parser.y} take over. This one is a bit more interesting
because the syntax of the @code{VARIABLES} section is more complicated
than that of the @code{DEVICES} section. But the basic principle is
the same: the lexer splits up the @code{EDL} code and feeds them to
the parser to "digest" them.


@code{fsc2} maintains a linked list of all variables and these list is
assembled from the code in the @code{VARIABLES} section. So this may be
a good place to give an introduction about how variables look like. All
variables are structures of type @code{Var}, which is declared (and
typedef-ed to @code{Var_T}) in the file @file{variables.h} (you may
prefer to look it up now). It contains a string pointer for the variable
name, a member for the type of the variable, an union for the value of
the variable (since there are several types of variables they can have
values of quit a range of types). Further, there are some data to keep
track of array variables (1- or multi-dimensional) and a member for
certain flags. Finally, there are pointers to allow the variable structure
to be inserted into a (doubly) linked list.

Before going into more details here's a list of the possible variable
types:
@example
UNDEF_VAR
STR_VAR
INT_VAR
FLOAT_VAR
INT_ARR
FLOAT_ARR
INT_REF
FLOAT_REF
INT_PTR
FLOAT_PTR
REF_PTR
FUNC
@end example
@noindent
Each variable begins its life with type @code{UNDEF_VAR}. But usually it
should become promoted to something more usful shortly afterwards, so
you will find it only in rare cases (it's sometimes used for temporary
variables, we're going to discuss them sometime later). A @code{STR_VAR}
is a variable holding a string, and also this type of variables only will
be found in temporary variables. What an @code{INT_VAR} and @code{FLOAT_VAR}
is will probably be quite obvious, these types of variables can hold a
single (long) integer or floating point (double) value, which are stored
in the @code{lval} and @code{dval} members of the @code{val} union of the
@code{Var} structure.


Variables of type @code{INT_ARR} and @code{FLOAT_ARR} are for holding
one-dimensional arrays of integer and floating point values.  For
variables of thess types the @code{len} field of the @code{Var}
structure will contain the (current) length of the array and the
members @code{lpnt} or @code{dpnt} of the @code{val} union are pointers
to an array with the data.


Variables of type @code{INT_REF} and @code{FLOAT_REF} are for
multidimensional arrays. These are a bit different because they don't
store any elements of the array directly but instead pointers to lower
dimensional arrays. These might again be multdimensional array variables
(but with one dimension less) or @code{INT_ARR} or @code{FLOAT_ARR}
variables, that then contain the data of a one-dimensional array. If
you have ben programming in e.g.@: Perl this concept will probably not
be new to you - there you also have one-dimensional arrays but which in
turn can have elements that are pointers to arrays. Otherwise, to make
clearer what I mean, lets assume that you define a 3-dimensional array
called @code{A} in the @code{VARIABLES} section:
@example
A[ 4, 2, 7 ];
@end example
@noindent
This will result in the creation of 9 variables. The top-most one (and
only that one can be accessed directly from the @code{EDL} script
because it's the only one having a name) is of type @code{INT_REF} and
contains an array of 4 pointers to 2x7-dimensional arrays, stored in the
@code{vprtr} member of the @code{val} union of the @code{Var} structure.
It's @code{dim} member is set to 3 since it's a 3-dimensional variable
and the @code{len} member gets set to 4 because the @code{val.vptr}
field is an array of 4 @code{Var} pointers.  Each of the 4 @code{Var}
pointers stored in the @code{val.vptr} field point to a different
variable, of which each is a again of type @code{INT_REF}. But these
variables pointed to will have a dimension of 2 only, so the @code{dim}
member is set to 2 and since each is of dimension @w{@code{[2, 7]}},
their @code{len} members are set to 2. And each of this lower-dimension
variables again will have the @code{val.vptr} array consist of (2)
pointers pointing to one.dimensional arrays, this time of type
@code{INT_ARR}. These @code{INT_ARR} variables, two levels below the
original variable named @code{A} will each contain an array of 7 integer
values, pointed to by @code{val.lpnt}.


When you count the variables actually created according to the scheme
above you will find that it are 9, one for the variable named @code{A}
itself, which in turn points to 4 newly created variables, of which each
again points to 2 further variables (which finally contain all the data
as one-dimensional arrays).


The remaining variable types @code{INT_PTR}, @code{FLOAT_PTR},
@code{REF_PTR} and @code{FUNC} are again only used with temporary
variables and will be discussed later.


The variables declared in the @code{VARIABLES} section are all elements
of a doubly linked list. The pointer to the top element is a member of
the global @code{EDL} variable. It's a structure of type @code{EDL_Stuff}
declared in @file{fsc2.h} and countaining data relevant for the @code{EDL}
script currently under execution. To find the first element of the list of
variables see the @code{EDL.Var_List} member. Directly beneath it you will
find that there's also a second variable named @code{EDL.Var_Stack}. This
variable is also doubly linked list of variables, but in contrast this
list is for temporary variables which get created and deleted all of the
time during the interpretation of an @code{EDL} script and is in the
following often referred to as the "stack". In this list also the types
of variables that were only mentioned @i{en passant} above can be found,
which I will shortly summarize here.


A variable of type @code{STR_VAR} gets created whenever in the text of
the @code{EDL} script a string is found or when an @code{EDL} function
returns a string. Since strings are always used shortly after their
creation (always within the statement they appear in) they are all
temporary variables. Variable of type @code{INT_PTR} and
@code{FLOAT_PTR} are variables in which the @code{val.lpnt} and
@code{val.dpnt} members point to arrays belonging to some other
variable, but never to the variable itself. Variables of type
@code{REF_PTR} are variables in which the @code{from} member (which
hadn't been mentioned yet) pointing to the variable it's pointing to.
Finally, variables of type @code{FUNC} have the @code{val.fcnt} member
pointing to address to one of the @code{C} functions that get called
for @code{EDL} functions.


After this detour about variables lets go back to what happens in
the @code{VARIABLES} section. In the most simple case the
@code{VARIABLES} section isn't much more than a list of variable names,
which need to be created. When the lexer finds something which looks
like a variable name (i.e.@: a word starting with a letter, followed by
more letters, digits or underscore characters), it will first check if a
variable by this name already exists by calling the function
@code{vars_get()} from @file{variables.h} with the name it found. It
either receives a pointer to the variable or @code{NULL} if the variable
does not exist yet. In the latter it will create a new variable by
calling @code{vars_new()} (which returns a pointer to the new
variable). It then passes the variables address to the parser. Assuming
the variable has been newly created it will still be of type
@code{UNDEF_VAR} and it's not clear yet if it's a simple variable or
going to be an array. Thus the parser asks the lexer for the next
token. If this is a comma or a semicolon it can conclude that the
variable is a simple variable and can set its type to either
@code{INT_VAR} or @code{FLOAT_VAR} (depending on its name starting with
a lower or upper case character) and is done with it. But if the next
token is a "@code{[}" the parser knows that this is going an
array and must ask the lexer for more tokens, which should be a list of
numbers, separated by commas and ending in a "@code{]}" (but there are
even more complicated cases). When all these have been read in the
parser calls some @code{C} code that sets up the new array according to
the list of sizes the parser received. More complicated cases may
include that instead of a number an asterisk ("@code{*}") is found, in
which case the array has to be initialized in a way to indicate that the
array hasn't been fully specified yet (this is done by setting the
@code{len} field of the variable structure for the array to 0 and
setting the @code{IS_DYNAMIC} flag in the @code{flags} member).


Other complications may include that a size if an array isn't given as a
number but as an arithmetic expression, possibly involving already
defined (and initialized) variables, arithmetic operators or even
function calls. In the hope not to bore you to death by getting too
detailed I want to describe shortly how the parser evaluates such an
epression because it's more or less the same all over the complete
program, not restricted to the @code{VARIABLES} section. Let's discuss
things using the following example
@example
abs( R + 5 * ( 2 - 7 ) )
@end example
@noindent
Here the lexer will first extract the "@code{abs}" token. Now I have to
admit a white lie I made above: I said that the lexer will check first
for tokens like this if it's an already existing variable. But it
actually first checks if it's an @code{EDL} function name, only if it
isn't it will check if it's a variable. And here it will find that
@code{abs} is a function by calling the function @code{func_get()} in
@file{func.c}. This function will return the address of a new temporary
variable on the stack (pointed to by @code{EDL.Var_Stack}) of type
@code{FUNC} with the @code{val.fcnt} holding the address of the function
to be executed for the @code{abs()} @code{EDL} function (which is
@code{f_abs()} in @file{func_basic.c}). The lexer now passes the address
of the variable on to the parser.


The parser knows that functions always have to be followed by an opening
parenthesis and thus will ask the lexer for the next token. If this
isn't a "@code{(}" the parser will give up, complaining about a syntax
error. Otherwise the parser has to look out for the function
argument(s), asking the lexer for more tokens. The next one it gets is a
pointer to the (hopefully already defined and initialized) variable
"@code{R}". But it doesn't know yet if this is already the end of the
(first) argument, so it requests another token, which is the "@code{+}".
From this the parser concludes that it obviously hasn't seen the end of
it yet and gets itself another token, the "@code{5}". A stupid parser
might now add the @code{5} to the value of @code{R}, but since the
parser knows the precedence of operators it has to defer this operation
at least until it has seen the next token. When the next token would be
a comma (indicating that a new function argument starts) or a closing
parenthesis it would now do the addition. But since the next token is a 
"@code{*}" it has to wait and first evaluate the "@code{(2 - 7)}"
part and multiply the result with @code{5} before again checking if it's
prudent to add the result to the value of @code{R}. Since the next token
the parser receives from the lexer is the "@code{)}, indicating the end
of the function arguments, it can go on, adding the result of
@w{"@code{5 * (2 - 7)}"} to the value of @code{R}. In this process
the temporary variable holding the pointer to the variable @code{R} gets
popped from the stack and a new variable with the result of the
operation is pushed onto the stack (i.e.@: is added to the end of the
linked list of variables making up the stack). Now the stack still
contains two variables, the variable pointing to the @code{f_abs()}
function and the variable with the function argument. And since the
parser has seen from the "@code{)}" that no more arguments are to be
expected for the function it will invoke the @code{f_abs()} function
with a pointer to the variable with the function argument.


If you cared to look it up you will have found that the @code{f_abs()}
function is declared as
@example
Var *f_abs( Var *v );
@end example
@noindent
This is typical for all functions that are invoked on behalf of
@code{EDL} functions: they always expect a single argument, a pointer to
a @code{Var} structure and always return a pointer to such a structure.
The pointer these functions receive is always pointing to the first
argument of the function. If the function requires more than one
argument it has to look for the @code{next} member of the variable,
and if this isn't @code{NULL} it points to the next argument. Of course,
zthis can be repeated until in the last argument the @code{next} field
is @code{NULL}. The function now has to check if the types of the
variables are what is required (it e.g.@: wouldn't make sense for the
@code{f_abs()} function if the argument would be a variable of type
@code{STR_VAR}) and if there are enough arguments (at least if the
function allows a variable number of arguments, if the function is
declared to accept only a fixed number of arguments these cases will be
dealt with before the function is ever called, see below).


The function now has to do it's work and, when it's done, creates
another temporary variable on the stack with the result (this is done by
a call of the function @code{vars_push()} in @file{variables.c}). In the
process it may remove the function arguments from the stack (using
@code{vars_pop()} also in @file{variables.c}), if it doesn't do so it
will be done automatically when the function returns. Note that there's
a restriction in that a function never can return more than a pointer to
a single variable, i.e. the variable pointed to must have its
@code{next} member set to @code{NULL}, being the last variable on the
stack. A function may also chose to return @code{NULL}, but it's good
practice to always return a value, if there isn't really anything to be
returned, i.e.@: the function always get invoked for its side effects
only, it should simply return an integer variable with a value of @code{1}
to indicate that it succeeded.


Again I have to admit that I wasn't completely honest when I wrote above
that "the parser invokes the @code{f_abs()} function". The parser does
not call the @code{f_abs()} function directly, but instead calls
@code{func_call()} in @file{func.c} instead with a pointer to the
variable of type @code{FUNC} pointing to the @code{f_abs()} function
(please remember that the function argument(s) are coming directly after
this variable on the stack). Before @code{func_call()} really calls
@code{f_abs()} it will first do several checks. The first one is to see
if the variable it got is really pointing to a function. Then it checks
how many arguments there are and compares it to the number of arguments
the function to be called is prepared to accept. If there are too many
it will strip off the superfluous one (and print out a warning), if there
aren't enough it will print out an error message and stop the
interpretation of the @code{EDL} script. If these tests show that the
function can be called without problems @code{func_call()} still has to
create an entry on another stack (the "call stack") that keeps track of
situations where during the execution of a function another function is
called etc., which is e.g.@: needed for emitting reasonable error
messages. Only then @code{f_abs()} is called. When @code{f_abs()}
returns, the @code{func_call()} first pops the last element from the
"call stack", automatically removes what's left of the function
arguments and the variable with the pointer to the @code{f_abs()}
function (always checking that the called function hasn't messed up
the stack in unrecoverable ways) before it returns the pointer with the
result of the call of @code{f_abs()} to the parser.


Now the parser will at last know what's the result of 
@example
abs( R + 5 * ( 2 - 7 ) )
@end example
@noindent
and can use it e.g.@: as the length of a new array.


Of course, beside getting defined new variables can also become
intialized in the @code{VARIABLES} section. The values used in the
initialization can, of course, also be the results of complicated
expressions. But these will be treated in exactly the same way as
already described above, the only new thing is the assignment part. The
parser knows that a variable is getting initialized when it sees the
"@code{=}" operator after the definition of a variable. It then parses
and interprets the right hand side of the equation and finally assigns
the result to the newly defined variable on the left hand side. To do so
it calls the function @code{vars_assign()} from @file{variables.c}
(if it's an initialization of an array also some other functions get
involved in the process).


The creation and initialization of one- and more-dimensional arrays
makes up a good deal of the code in @file{variables.c}. Unfortunately,
so many things have to be taken care of that it can be quite a bit of
work understanding what's going on and I have to admit that it usually
also takes me some time to figure out what (and why) I have written
there, so don't worry in case you have problems understanding everything
at the first glance...


But now let's get back to the main theme, i.e.@: what happes during the
interpretation of an @code{EDL} script. I guess most of what can be said
about the @code{VARIABLES} section has been said and we can assume that
we reached the end of this section.  The lexer generated from
@file{vars_lexer.l} will then return to @code{section_parser()} in the
lexer created from @file{split_lexer.l} with a number indicating the
type of the next section.


If the @code{EDL} script is for an experiment where pulses are used
chances are high that the next sections will be @code{ASSIGNMENTS} and
@code{PHASES} section. But I don't want to go into the details of the
handling of these sections. In principle, things work exactly like in the
interpretation of the @code{VARIABLES} section, i.e.@: there's again a
lexer for each section (generated from @file{assign_lexer.l} and
@file{phases_lexer.l}) and a parser (generated from @file{assign_parser.y}
and @code{phases_parser.y}), which work together to digest the @code{EDL}
code. The interesting things happening here is the interaction with the
module for the pulser, but this is in large parts already covered by the
second half of next chapter about writing modules.


The next section is usually the @code{PREPARATIONS} section. And again
nothing much different is going on here from what we already found in
the @code{VARIABLES} section: the lexer and parser generated from
@file{preps_lexer.l} and @file{preps_parser.y} play their usual game,
one asking the other for tokens and then trying to make sense from them,
analyzing the sequence of tokens and executing the appropriate actions.
The only difference is that the syntax is a bit different from the one
of the @code{VARIABLES} section, otherwise the same lexer and parser
could be used.


Where things again get interesting is with the start of the
@code{EXPERIMENT} section. Here @code{fsc2} does not immediately
interpret the @code{EDL} code as it has been doing in all the other
sections up until now. You already may notice from what files you find:
while there exists a file @file{exp_lexer.l} there are two parsers,
@file{exp_test_parser.y} and @file{exp_run_parser.y}. And at first,
these parsers even don't get used. Instead, only the lexer is used to
split the @code{EXPERIMENT} section into tokens and functions from
@file{exp.c} store the tokens in an array of structures (of type
@code{Prg_Token}, see @file{fsc2.h}).


There are several reasons for storing the tokens instead of executing
statements immedately. But the main point is that the @code{EXPERIMENT}
section isn't interpreted only once but at least two times (or even more
often if the same experiment is run repeatedly), and parts of the
@code{EXPERIMENT} section may even be repeated hundreds or thousands of
times (the loops in the @code{EXPERIMENT} section). Now, as I already
mentioned above, @code{fsc2} isn't interpreting the @code{EDL} script
itself, but a "predigested" version that has been run through the
@code{fsc2_clean} utility.


Of course, the question not answered yet is why it's done this way. And
the answer is simplicity and robustness (and, of course, my lazyness).
An @code{EDL} script can contain comments, may include include further
@code{EDL} scripts using the @code{#INCLUDE} directive etc. If this
wouldn't be dealt with by a the @code{fsc2_clean} utility each and every
section lexer would have contain code for removing comments and for
dealing with inclusion of other @code{EDL} scripts (which isn't
trivial), making the whole design extremely complicated and thus
error-prone. By moving all of these tasks into a single external utility
a lot of potential problems simply disappear.


But one has to pay a price. And this is that we can't simply jump back
in a file to a certain statement in the @code{EDL} script (because
there's no file to move around in, but just a stream of data that gets
read from an external utility). On the other hand, when you have to
repeatedly interpret parts of the script you have to jump back to be
able to interpret the same code over and over again. One solution would
be to store the "predigested" @code{EDL} the program received from the
@code{fsc2_clean} utility in memory and then make the lexer split it
into tokens again and again when needed. But this would be a wast of CPU
time when you can store the tokens of the the code instead, which then
can be feed to the parser again and again without the need for a tokenizer.


So, why to repeat some or all code of the @code{EXPERIMENT} section at
all? First of all, before the experiment is run the code should be
checked carefully. It's much better to find potential problems at an
early stage instead of having an experiment stop after it has run a for
a long time just because of an easy to correct error which could have
been detected much earlier. (Just imagine how happy you would be if you
had run an experiment for 24 hours on a difficult to prepare sample,
already seeing from the display that that's going to become an important
part of your PhD thesis but then the program suddenly stops before it
finally stores the data to a file because in the code for storing the
data there's a syntax error...)


Thus, each @code{EDL} script needs to be checked. And to do so, it must
have been read in completely before the experiment is started. And
another point is that an experiment may have to be repeated. Of course,
the whole @code{EDL} script could be read in again when an experiment is
restarted. But this would also require testing it again (which might
take quite a bit of time), so it's faster to work with the already
tested code.


And that's why the tokens of the @code{EXPERIMENT } section are stored
in memory e.g.@: in an array. It's done in the function @code{store_exp()}
in @file{exp.c} (which is called from the @code{C} code in
@file{exp_lexer.l}). The function repeatedly calls the lexer generated
from @file{exp_lexer.l} for new tokens, storing each one in a new
structure, until the end end of the @code{EDL} code is reached. The
array of structures is pointed to by @code{EDL.prg_token}. While it does
so it already runs some simple checks, e.g. for unbalances parentheses
and braces.


When all tokens have been stored the function calls @code{loop_setup()}.
The function initializes loops and @code{IF}-@code{ELSE} constructs.
Take as an example a @code{FOR} loop. To later be able to find out where
the statements of the body of the loop starts a pointer to the first
token of the loop body is set in the structure for the @code{FOR}
token. And since, at the end of the @code{FOR} loop, control needs to be
transfered to the first statement after the loop body also a pointer
pointing to the first token after the loop also is set. And for a
keyword like @code{NEXT} a pointer to the start of the loop it belongs
to needs to be set. Finding the start and the end of loops is simply
done by counting levels of curly braces, '@code{@{}' and '@code{@}}'
(that's why the statements of loops and also @code{IF} constructs must
be enclosed in curly braces, even if there's only a single statement).


This task out of the way the first real type of test can be done. This
is still not what in the rest of the manual is called the "test run" but
just a syntax check of the @code{EXPERIMENT} section. For this purpose
there exists a parser, generated from @file{exp_test_parser.y} that does
not execute any actions associated with the statements of the
@code{EXPERIMENT} section. It is only run to test if all statements are
syntactically correct. The parser itself need some instance that feeds
it the tokens. Since there's now no lexer (all tokens have already been
read in and stored in the array of tokens), the function
@code{exp_testlex()} in @code{exp.c} plays the role the lexers had in
the other sections: each time it's called it passes the next token from
the array back to the parser until it hits the end of the token array.


Only when this syntax check succeeded the real test run is started.
From the @code{C} code in @file{exp_lexer.l} the function
@code{exp_test_run()}, again from @file{exp.c}, is called.  But before
the test run can really start a bit of work has to be done.  Some of the
variables in the @code{EDL} script may have already been set during the
sections before the @code{EXPERIMENT} section and when the real
experiment gets started, they must be in the same state as they were
before the test run was started. But since they will usually will be
changed during the test run all @code{EDL} variables (i.e.@: all
variables from the variable list pointed to by @code{EDL.Var_List}) must
be saved, which is done in the function @code{vars_save_restore()} in
@file{variables.c}.


Then in all modules a hook function has to be called (at least if the
module defines such a function). This gives the modules e.g.@: a chance
to also save the states of their internal variables. All of the test
hook functions are called from the function @code{run_test_hooks()}
from @file{loader.c}.


This preparations successfully out of the way the test run can finally
start. To tell all parts of the program that get involved in the test
run that this is still the test run and not a real experiment the member
@code{mode} of the global structure @code{Internals} is set to a value
of @code{TEST}. When you look through the code of the @code{C} functions
called for @code{EDL} functions, both in @code{fsc2} itself and in the
modules, you will find, that @code{Internals.mode} is again and again
tested, either directly or via the macro @code{FSC2_MODE} (see
@file{fsc2_module.h} for its definition). They do so because some things
can or should only be done during the real experiment. E.g.@: all
modules must refrain from accesssing the devices they are written to
control because at this stage they aren't initialized yet. And also
other functions like the ones for graphics aren't supposed to really
draw anything to the screen yet. So everything these functions are
supposed to do during the test run is to check if the arguments they
receive are reasonable and then return some also reasonable values.


Instead of the parser for the mere syntax check now the "real" parser,
generated from @file{exp_run_parser.y} gets involved. It's the real
thing, executing the code associated with the @code{EDL} statements
instead of just testing syntactical correctness. And it also needs some
instance feeding it tokens. This is now the function
@code{deal_with_tokens_in_test()}. When you compare it to
@code{exp_testlex()} that was used during the syntax check, you will
find that it's a bit more complicated, resulting from the necessity to
execute flow control statements, which had not to be done during the
syntax check and which the parser does not take care of.


So the function @code{deal_with_tokens_in_test()} calls the parser
whenever a non-flow-control token is teh current token. The parser
itself calls @code{exp_runlex()} whenever it needs another token.
@code{exp_runlex()} stops the parser when it hits a flow control token
by returning 0 (which a parser interprets as end of file). This brings
us back into @code{deal_with_tokens_in_test()} which now does what's
required for flow control. This especially includes checking the
conditions of loops and @code{IF}-@code{ELSE} constructs. For testing
conditions the function @code{test_condition()} is called, which invokes
a special parser generated from @file{condition_parser.y} and made for
this purpose only and that requests new tokens by calling the function
@code{conditionlex()}, which also can be found in @code{exp.c}. When the
condition has been checked @code{test_condition()} returns a value
indicating that the condition is either satisfied or not and the code
in @code{deal_with_tokens_in_test()} can decide from the return value
how to proceed. This takes care that all loops are repeated as often as
they should and that in @code{IF}-@code{ELSE} constructs the correct
path through the @code{EDL} code is taken.


All the above will be done until we either reach the end of the array of
tokens or one of the @code{EDL} functions called in the process signals
an unrecoverable error. I have spend so much space with explaining all
this because the way the code in the @code{EXPERIMENT} section is
executed during the experiment is basically identical to the way it is
done during the test run.


There are only a few things left that might be of interest when you try
to understand what's happening in @code{exp.c} and the related parsers.
First of all, there's one token that does not get stored in the array of
tokens. This is the @code{ON_STOP:} label. When during the experiment
the @code{Stop} button gets pressed by the user flow of control is
passed as soon as possible to the code directly following the label.  As
a label, it isn't something that can be executed, so it isn't included
into the array of tokens. Instead in the global variable
@code{EDL.On_Stop_Pos} the position of the first token following the
@code{ON_STOP:} label is stored, so that the parts of the program taking
care of flow control can calculate easily where to jump to when the user
hits the @code{Stop} button.


The second point I have only mentioned @i{en passant} is error
handling. You will perhaps have already noticed that there seems to be
only a rather limited amount of error checking, but that on the other
hand there are some strange constructs in the @code{C} code with
keywords like @code{TRY}, @code{TRY_SUCCESS}, @code{CATCH()},
@code{OTHERWISE} or @code{THROW()}. If you have some experience with
@code{C++} some of the keywords will probably ring a bell, but for
@code{C} programmers they look rather strange.


In @code{C} errors are usually handled by passing back a return value
from functions indicating either success or failure (and possibly also
the kind of problem). This requires that for most function calls it must
be tested if the function succeeded and if not the function that called
the lower level function must either try to deal with the error or, when
it isn't able to do so, must escalate the problem by returning itself a
value that indicates the type of problems it run into. This requires
lots of discipline by the programmer because she has to explicitely
write error checking code over and over again and also makes the source
often quite hard to read since what really gets done in a function
becomes drowned in error checking code. And when an error happeningin a
very low level function that's hard to deal with it may happen that
control has to transfered to a function serveral levels above that
finally takes care of the problem, which might make figuring out what
will happen on such errors hard to figure out.


@code{C++} has a concept of error handling which is very elegant when
compared to how it's usually is done in @code{C}, the notion of
exceptions, which usually are seen as error conditions (but could also
be used for other unusual conditions). A function can declare itself
responsible for a certain type of expections by executing some block of
code, where this exception might be triggered, within a @i{try}-block
and after the end of the @i{try}-block @i{catch} the exception. That
means that when the exception happens (gets thrown) control is
transfered immediately to the code in the block of code following the
@i{catch} without functions on the intermediate levels having to get
involved. So throwing an exception results in priciple in a non-local
jumo from the place where the exception got thrown to the code in the
@i{catch} block. The idea of non-local jumps is a bit alien to most
@code{C} programmers because, when one uses the infamous @code{goto} at
all, it can only be used to jump within the code of a function (and even
then only with some restrictions). But there's a pair of (rarely used)
"functions" in @code{C} that allow such non-local jumps, @code{setjmp()}
and @code{longjmp()}. And these function can be used to cobble together
some poor mans equivalent of the @i{try}, @i{throw} and @i{catch}
functionality of @code{C++} with a set of macros and functions. Of
course, it's not as polished as its big brother from @code{C++}, it's
more difficult to use, more error prone and also much more restricted,
but it still can make life a bit simpler when compared to the usual way
error handling is done in @code{C}.


I don't want to go into details on how exactly it works, you will find
the code for it in @file{exceptions.h} and @file{exceptions.c} and I
also will refrain from telling you here how it is used because that's
already documented in the chapter on writing device modules. I just want
to give an example how it's used in @code{fsc2}. When you again look up
the function @code{section_parser()} in the the primary lexer,
@file{split_lexer.l}, you will find that the whole code in this function
is enclosed in a block starting with the macro @code{TRY}, thus making
it the final place where every problem not handled by the lower level
function will end up. Now one type of error checking you will find all
over the place in (well-written) @code{C} code is checking the return
value of functions for memory allocation. But this isn't done in
@code{fsc2} (except when the function doing the allocation is willing to
deal with problems). Throughout the whole program instead of e.g.@:
@code{malloc()} the function @code{T_malloc()} is used. And this
function, which is just a wrapper around the @code{malloc()} call, does
throw an @code{OUT_OF_MEMORY_EXCEPTION} if its call of @code{malloc()}
fails. Unless the function calling @code{T_malloc()} catches the
exception it gets escalated to the @code{section_parser()} function,
thereby effectively stopping further interpretation of the @code{EDL}
script. The same happens for other types of exceptions, for example most
of the functions associated with @code{EDL} functions (both the built-in
functions and the functions in modules) usually print an error message
and then throw an @code{EXCEPTION} to indicate that they got a problem
that requires a premature end of the interpretation of the @code{EDL}
script.


But stopping the interpretation of the script isn't always necessary,
sometimes there are only potential problems the user should be made
aware of or things that are rather likely to be errors but also could
require only a warning. In these cases the function that should be
used to print out warnings and error messages, @code{print()} from
@file{util.c}, will help keeping track of the number of times this
happened. @code{print()} is, if seen from the user perspective, more
or less like the standard @code{printf()} function, only with an
additional argument, preceeding the arguments one would pass to
@code{printf()}. This additional argument is an integer indicating
the severity of the problem and can be either @code{NO_ERROR},
@code{WARN}, @code{SEVERE} or @code{FATAL} (with the obvious
meanings). @code{print()} will now do a few additional things: it will
first increment a counter for the different types of warnings (these
counters are in @code{EDL.compilation}) and then prepend the message
to be printed out with information about the name and the line number
in the @code{EDL} script that led to the problem and also, if
appropriate, the @code{EDL} function the problem was detected in.
Finally it writes the message into the error browser in @code{fsc2}s
main form.


After all this talk about error handling lets get back to the bright
side of life: perhaps without you noticing we have nearly reached the
end of the test run. All that remains to be done under normal conditions
is to restore the values of all of the @code{EDL} variables in
@code{EDL.Var_List} (which, as you will remember, got stashed away
before the test run got started) and call hook functions via
@code{run_end_of_test_hooks()} in @file{loader.c} for all modules that
contain a function to be run at the end of a test run. Afterwards
control will be transfered back to the @code{main()} function in
@file{fsc2.c}, which will now wait for the user to start the experiment
(of course unless the user initated the test run by pushing the
@code{Start} button, in which case the experiment will be started
immediately).


When the experiment is to be started the function @code{run_file()} in
@file{fsc2.c} is invoked. Its main purpose is to ask the user if she is
serious about starting the experiment even if in the test run it was
found that there were some things that required a warning or even a
severe warning. If there weren't or the user doesn't care about the
warnings then the function @code{run()} in @code{run.c} is called, which
is where the interesting stuff happens.


The first thing the function has to do is to test the value of the
global variable @code{EDL.prg_length}, which normally holds the number
of tokens stored during the analysis of the @code{EXPERIMENT} section in
the array of tokens, @code{EDL.prg_token}. But when it's set to a
negative value there's no @code{EXPERIMENT} section at all in the
@code{EDL} script, so no experiment can be done. The next step is to
intialize the @code{GPIB} bus, at least if one or more of the modules
indicated that the devices they are controlling are accessed via this
interface. Afterwards we again have to check the value of
@code{EDL.prg_length}. If it is 0 this means that there was an
@code{EXPERIMENT} section label but no code following it. This is
usually used when people want to get the devices into the state they
would be at the start of the experiment (e.g.@: for setting a certain
pulse pattern in a pulser or going to a certain field position), but
don't want to run a "real" experiment yet. So we should honor this
request and call @code{no_prog_to_run()} in this case.


In @code{no_prog_to_run()} the hook functions in the modules to be
executed at the start of an experiment (via @code{run_exp_hooks()} in
@file{loader.c}) are called. These are responsible for bringing the
devices into their initial states. Then we're already nearly done and
call via @code{run_end_of_exp_hooks()} another set of hook functions,
the ones that are to be executed at the end of an experiment. Now the
@code{GPIB} bus can be released and device files for serial ports that
got opened are closed in case the module that opened them should have
forgotten to do so. And that's already the end of this miniml kind of
experiment.


For a real experiment more exciting things happen, started by calling
@code{init_devs_and_graphics()}. Of course, also here the hook functions
to be run at the start of an experiment in all modules get called. Then
the new window for displaying the results of the experiment is created,
involving the initialization of all kinds of variables for the graphics.
This is done by a call of the function @code{start_graphics()}, which
you will find in @file{graphics.c}.


Then we must prepare for the program splitting itself in two separate
processes, one for running the experiment and one for dealing with the
interaction with the user. This requires setting up channels of
communication between the two processes by calling @code{setup_comm()}
from @file{comm.c}. Since the communication between the processes is
quite important I would like to spend some time on this topic.


All communication between parent and child is controlled via a shared
memory segment. It is a structure of type @code{MESSAGE_QUEUE}, declared
in @file{comm.h}. This structure consists of an array of structures of
type @code{SLOT} and two marker variables, @code{low} and @code{high}.
When the child needs to send data to the parent (there's no sending of
data from the parent to the child that isn't initialized by the child)
it sets the @code{type} field of the @code{SLOT} structure indexed by
the @code{high} marker (which it increments when the message has been
assembled) to the values @code{DATA_1D} or @code{DATA_2D}. It then
creates a new shared memory segment for the data and then puts the key
of this shared memory segment into the @code{shm_id} field of the
slot. Whenever it has time the parent checks the values of the
@code{high} and the @code{low} marker and, if they are different, deal
with the new data, afterwards incrementing the @code{low} marker. Both
markers wrap around when they reach the number of avalailable
@code{SLOT} structures.


To keep the child process from sending more messages than there are free
slots in the shared array of @code{SLOT} structures there's a semaphore
that gets initialized to the number of available slots and that the child
process has to wait on (thereby decrementing it) before using a new
slot. The parent, on the other hand, will post (i.e.@: increment) the
semaphore each time it accepted a message from the slot indexed by the
@code{low} marker, thus freeing the slot.


Beside data the child also may need to send what's called "requests" in
the following. These requests always require an answer by the parent.
In this case the type field of the @code{SLOT} structure is set by the
child to the value @code{REQUEST}, indicating that this is a request. The
data exchange between parent and child for requests is not done via
shared memory segments but by using a simple set of pipes, both for the
data making up the request from the child as well as the reply by the
parent. A request will induce the parent to listen on the pipe and,
depending on the type of the request, to execute some action on behalf
of the child. It then either returns data collected in the process or
just an acknowledgment, telling the child process that it's done and
can continue with its work. Because the child has always to wait for a
reply to its request there never can be more than a single request in
the message queue.


After having run the start-of-experiment handlers in all modules,
initializing the graphics and successfully setting up the communication
channels the parent still has to set up a few signal handlers. One
signal (@code{SIGUSR2}) will be sent by the child to the parent when
it's about to exit and must be handled. And also for the @code{SIGCHLD}
signal a special handler is installed during the experiment. This also
out of the way, the parent finally forks to create the child process
resonsible for running the experiment. From now on we will have to
distinguish carefully about which process we're talking.


If the call of @code{fork()} succeeded, the parent process just has to
continue to wait for new events triggered by the user (e.g.@: by
clicking on one of the buttons) and to reguarly check if new data from
the child have arrived. The latter is done from within an idle handler,
a function invoked whenever the parent process isn't busy. The function
is called @code{new_data_callback()} and can be found in @file{comm.c}.
But most of the actual work, i.e.@: accepting and displaying the data,
is done in the function @code{accept_new_data()} in @file{accept.c}.


The child process will have itself initialized in the mean time in the
function @code{run_child()} in @file{run.c}. It closes the ends of pipes
it doesn't need anymore and sets up its own signal handlers. Then its
main work starts by invoking the function @code{do_measurement()} where,
just as already described above for the test run, the stored tokens from
the @code{EXPERIMENT} section of the @code{EDL} script get interpreted.
Again for tokens not involved in flow control the parser created from
@file{exp_parser.y} is used. This parser gets passed tokens delivered
from the array of stored tokens by the function @code{exp_runlex()} in
@file{exp.c} (the same one as used in the test run), executing the code
associated with the sequences of tokens. And again tokens for flow control
are not dealt with by the parser but by the code in
@code{deal_with_program_tokens()}.


The most notable differences to the test run is that the member
@code{mode} of the global structure @code{Internals} is now set to a
value of @code{EXPERIMENT}, which is tested all over the program and the
modules, either directly or via the macro @code{FSC2_MODE}. The other
difference is that now it is always tested if the child process got a
signal from the parent process, telling it to quit (this happens if the
member @code{do_quit} of the global structure @code{EDL} is set).  In
this case the flow of control has to be transfered immediately (or, to
be precise, immediately after the parser has interpreted the current
statement) to the code following after the @code{ON_STOP} label.


When all the code from the @code{EDL} script has been executed the
function @code{do_measurement()} returns to @code{run_child()}.  Here
the child exit hook functions in all modules is executed.  These
functions are run within the context of the child process and shouldn't
be confused with the exit hook functions that are executed in the
context of the parent process when the module gets unloaded.  Then the
child process sends the parent a signal to inform it that it's going to
exit and does so after waiting for the parent to send it another signal.


After this tour the force throught the childs code lets take a closer
look at the interactions between the child and the parent. Most
important is, of course, the exchange of data between the child and
parent. We already mentioned above how this is done, i.e.@: via a shared
memory segment or a set of pipes. Now lets investigate the way data and
requests are formated a bit further.


As already has been mentioned, every data exchange is triggered by the
child process, which puts a @code{SLOT} structure in to the message
queue, residing in shared memory, and increments the @code{high}
marker of the message queue. The @code{type} member of the @code{SLOT}
structure is set to either @code{DATA_1D}, @code{DATA_2D} or
@code{REQUEST}. Messages of type @code{DATA_1D} and @code{DATA_2D} are
messages related to drawing new data on the screen, either in the window
for for one- or two-dimensional data. Messages of @code{REQUEST} are
messages that ask the parent to do something on behalf of the child
process (e.g.@: asking the user to enter a file name, click on the
button in an alert message, create, modify or delete an element in the
tool box etc.) and always require a reply by the parent.


In the @code{accept_new_data()} function new messages of the types
@code{DATA_1D} and @code{DATA_2D} are taken from the message queue and
the corresponding functions get called. Because a message can consist of
more than one set of data (e.g.@: when in the @code{EDL} function
@code{display_1d()} new data are to be drawn for more than one curve,
there would be one set for each curve). Thus the first bit of
information in the data set in shared memory and indexed by the key
(member @code{shm_id}) in the @code{SLOT} structure is the number of
data sets. Should this number be negative it means that the message
isn't meant for the @code{EDL} functions @code{display_1d()} or
@code{display_2d()} (i.e.@: the functions for drawing new data on the
screen) but for one of the functions @code{clear_curve()},
@code{change_scale()}, @code{change_label()}, @code{rescale()},
@code{draw_marker()}, @code{clear_marker()} or @code{display_mode()}
(either the 1D or 2D version of the function, depending if the data type
was @code{DATA_1D} or @code{DATA_2D}). The messages are dealt with in
the function @code{other_data_request()} in @file{accept.c}, which then
calls the appropriate functions in the parts of the program responsible
for graphics (which are @file{graphics.c}, @file{graph_handler_1d.c},
@file{graph_handler_2d.c} and @file{graph_cut.c}).


In contrast, for data packages with a positive number of sets, the
functions @code{accept_1d_data()} or @code{accept_2d_data()} (also in
@file{accept.c}) get invoked for each of the data sets. It s the
resonsibility of these functions to insert the new data into the
internal structures maintained by the program for the data currently
displayed. When the functions are done with a data set these internal
structures must be in a state that the next redraw of the canvas for
displaying the data will result in the new data becoming displayed
correctly. Explaining this in detail would require to also explain the
whole concept of the graphics in @code{fsc2}, which I am not going to do
here. If you want to know more about it you will find the source code
associated with graphics in the already mentioned files
@file{graphics.c}, @file{graph_handler_1d.c}, @file{graph_handler_2d.c}
and @file{graph_cut.c}. Keep in mind that this code will never be
executed by the child process but only by the parent.


Now about the handling of requests: when in @code{accept_new_data()} a
message of type @code{REQUEST} is found control is passed back to the
calling function, @code{new_data_handler()} in @file{comm.c}, which
then invokes @code{reader()}. Within this function the parent process
reads on its side of the pipe to the child process. The child process
will write a structure of type @code{CommStruct} (see @file{comm.h}) to
the pipe. This structure contains a @code{type} field for the kind of
request and a union, which in some cases might already contain all
information associated with the request. Otherwise, the child will also
have to send further data via the pipe to the parent. The layout o
these additional data depends strongly on the type of the request and
you will have to look up the functions that initialize the request to
find out more about it.


Writing of the data to the pipe is done by the child via the
@code{writer()} function in @file{comm.c}. When the child process has
successfully written its data to the pipe it must wait for the parent to
reply by listening on its read side of the pipe by calling
@code{reader()}. In the mean time the parent will execute whatever
actions are associated with a request and then call @code{writer()} to send
back either just an acknowledgment or a set of data to the child process
waiting in @code{reader()} for the reply.


Another thing that might get done while the parent process is running
its idle handler and if there aren't any more data by the child process
to be dealt with is checking if the HTTP server, that might have gotten
switched on by the user is asking for either information about the
current state of @code{fsc2} or a file with a copy of what's currently
displayed on the screen. This is done in the function @code{http_check()}
in @file{http.c}. Please note that never more than a single request by
the HTTP server is serviced to keep @code{fsc2} from being slowed down
too much during the experiment from a large number of such requests.


When the child exits the parent has again to do a bit of work. It
deletes the channels of communication with the child process, i.e.@:
closes its remaining ends of the pipes and removes the shared memory
segments and the semaphore used to protect the message queue (after
having worked its way through all remaining messages). Then it deletes
the tool box (if it was used at all) end runs the end-of-experiment hook
functions of the modules. If the GPIB bus or serial ports were used the
appropriate function in the GPIB library is called to close the
connection to the bus and also the device files for the serial ports are
closed.


If the user now closes the window(s) for displaying the data the program
is in a state that allows a new experiment to be started. This can be
done by repeating the same experiment, in which case the already stored
tokens would be interpreted again, i.e.@: the @code{EDL} script would't
have to be read in and tested again but the program would immediately go
to the start of the @code{EXPERIMENT} section. But, of course, a
completely new experiment can be done by loading a new @code{EDL}
script.



@node Coding conventions, , Reading the sources, Internals
@section Coding conventions


When you try to read the source it might be helpful to know about a few
conventions I try to follow. This concerns mostly the names and spelling
of variables. And while I try to be consistent unfortunately I don't always
follow my own rules all of the time and thus later have to try to to correct
these mistakes. I can't promise that I am always successful in this endeavor.


Local variables with function scope should always have all lower case,
i.e.@: @code{num_points}. For local variables with file scope (i.e.@:
variables that are global to a whole C file but not further) I tend
to use an upper case for the very first character of the name and all others
in lower case, i.e.@: @code{Child_return_status}. Truely global variables
(there are more than I am happy with but it's basically impossible to throw
out all of them) mostly consist of more than a single word, where I use an
underscore character between the words, and have each of the words start
with an upper case letter, i.e.@: @code{Function_Names}. Sometimes that
rule gets a bit loosened up when one of the words is an acronym, where I
then use all upper case characters for the word.


When I use @code{typedef}'ed types I use a similar convention as for global
variables but append the suffix @code{_T} to the end of the name to make them
stick out, like in @code{GUI_Stuff_T}. For @code{#define}'d constants I try
to use all upper case, e.g.@: @code{NUM_CHANNEL_NAMES} - the same holds for
enumerated values.


With a few exceptions function names are always in all lower case.


Of course, these conventions only hold for my own variables etc. When I use
symbols from libraries I have to use what I get, so e.g. @code{XPoint} is
some @code{typedef}'ed type from X, even though it looks a bit like a global
variable. But I didn't want to go as far as redefining everything else
because it would make things unreasonably difficult to read for people
with experience with the other libraries.
